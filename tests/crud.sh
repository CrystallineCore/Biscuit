#!/bin/bash

#===============================================================================
# FIXED Biscuit Index CRUD Latency Benchmark
# 
# KEY FIX: Reuses database connections to measure actual cached performance
# instead of repeatedly paying index rebuild costs
#
# Comprehensive comparison of:
#   - No Index (baseline)
#   - B-tree Index
#   - pg_trgm GIN Index
#   - Biscuit Index
#
# Tests: INSERT, UPDATE, DELETE, and SELECT latency across various scenarios
#===============================================================================

set -e  # Exit on error

# Configuration
DBNAME="postgres"
DBUSER="postgres"
DBHOST="${PGHOST:-localhost}"
DBPORT="${PGPORT:-5432}"

# Test parameters - FIXED for statistical validity
# For quick testing, use smaller values. For production benchmarks, increase these.
WARMUP_ITERATIONS=10
TEST_ITERATIONS=20
RECORD_COUNT=10000
BATCH_SIZES=(1 10 100 1000)

# Output files
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="benchmark_results_${TIMESTAMP}"
CSV_FILE="${OUTPUT_DIR}/crud_latency.csv"
REPORT_FILE="${OUTPUT_DIR}/report.txt"
CHART_FILE="${OUTPUT_DIR}/chart.html"
TEMP_SQL="${OUTPUT_DIR}/queries.sql"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

#===============================================================================
# Utility Functions
#===============================================================================

detect_auth_method() {
    if sudo -u postgres psql -d postgres -c "SELECT 1;" > /dev/null 2>&1; then
        log "Using peer authentication (sudo -u postgres)"
        AUTH_METHOD="peer"
        return 0
    fi
    
    if [ -n "$PGPASSWORD" ]; then
        if psql -h "$DBHOST" -p "$DBPORT" -U "$DBUSER" -d postgres -c "SELECT 1;" > /dev/null 2>&1; then
            log "Using password authentication with PGPASSWORD"
            AUTH_METHOD="password"
            return 0
        fi
    fi
    
    if [ -f "$HOME/.pgpass" ]; then
        if psql -h "$DBHOST" -p "$DBPORT" -U "$DBUSER" -d postgres -c "SELECT 1;" > /dev/null 2>&1; then
            log "Using password authentication with .pgpass"
            AUTH_METHOD="password"
            return 0
        fi
    fi
    
    error "Cannot connect to PostgreSQL"
    error ""
    error "Please set up authentication using ONE of these methods:"
    error ""
    error "METHOD 1: Set PGPASSWORD environment variable"
    error "  export PGPASSWORD='your_password'"
    error "  ./benchmark_crud_fixed.sh"
    error ""
    error "METHOD 2: Create ~/.pgpass file"
    error "  echo 'localhost:5432:postgres:postgres:your_password' > ~/.pgpass"
    error "  chmod 600 ~/.pgpass"
    error "  ./benchmark_crud_fixed.sh"
    error ""
    error "METHOD 3: Configure PostgreSQL for peer authentication"
    error "  Edit /etc/postgresql/*/main/pg_hba.conf"
    error "  Add: local   all   postgres   peer"
    error "  sudo systemctl restart postgresql"
    error "  sudo ./benchmark_crud_fixed.sh"
    error ""
    exit 1
}

log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

exec_sql() {
    if [ "$AUTH_METHOD" = "peer" ]; then
        sudo -u postgres psql -d "$DBNAME" -t -c "$1"
    else
        psql -h "$DBHOST" -p "$DBPORT" -U "$DBUSER" -d "$DBNAME" -t -c "$1"
    fi
}

exec_sql_file() {
    if [ "$AUTH_METHOD" = "peer" ]; then
        sudo -u postgres psql -d "$DBNAME" -f "$1"
    else
        psql -h "$DBHOST" -p "$DBPORT" -U "$DBUSER" -d "$DBNAME" -f "$1"
    fi
}

# CRITICAL FIX: Execute SQL in persistent connection and extract timing
exec_sql_persistent() {
    local sql_file="$1"
    local result_file="$2"
    
    if [ "$AUTH_METHOD" = "peer" ]; then
        sudo -u postgres psql -d "$DBNAME" -f "$sql_file" 2>&1 | grep "Time:" | awk '{print $2}' | sed 's/ms//' > "$result_file"
    else
        psql -h "$DBHOST" -p "$DBPORT" -U "$DBUSER" -d "$DBNAME" -f "$sql_file" 2>&1 | grep "Time:" | awk '{print $2}' | sed 's/ms//' > "$result_file"
    fi
}

calc_stats() {
    local file="$1"
    awk '
    {
        values[NR] = $1
        sum += $1
        sumsq += ($1)^2
    }
    END {
        n = NR
        if (n == 0) {
            print "0.00,0.00,0.00,0.00,0.00,0.00"
            exit
        }
        
        mean = sum / n
        variance = (sumsq / n) - (mean^2)
        stddev = sqrt(variance > 0 ? variance : 0)
        
        for (i = 1; i <= n; i++) {
            for (j = i + 1; j <= n; j++) {
                if (values[i] > values[j]) {
                    tmp = values[i]
                    values[i] = values[j]
                    values[j] = tmp
                }
            }
        }
        
        p50_idx = int(n * 0.50)
        p95_idx = int(n * 0.95)
        p99_idx = int(n * 0.99)
        
        if (p50_idx < 1) p50_idx = 1
        if (p95_idx < 1) p95_idx = 1
        if (p99_idx < 1) p99_idx = 1
        if (p50_idx > n) p50_idx = n
        if (p95_idx > n) p95_idx = n
        if (p99_idx > n) p99_idx = n
        
        min_val = values[1]
        
        printf "%.2f,%.2f,%.2f,%.2f,%.2f,%.2f\n", 
               mean, stddev, min_val, values[p50_idx], values[p95_idx], values[p99_idx]
    }
    ' "$file"
}

#===============================================================================
# Setup Functions
#===============================================================================

setup_database() {
    log "Setting up database: $DBNAME"
    
    exec_sql "DROP TABLE IF EXISTS test_data CASCADE;" 2>/dev/null || true
    
    exec_sql "CREATE EXTENSION IF NOT EXISTS pg_trgm;"
    exec_sql "CREATE EXTENSION IF NOT EXISTS biscuit;"
    
    exec_sql "
    CREATE TABLE test_data (
        id SERIAL PRIMARY KEY,
        name TEXT,
        email TEXT,
        description TEXT,
        created_at TIMESTAMP DEFAULT NOW()
    );
    "
    
    log "Database setup complete"
}

# FIXED: Generate data ONCE with fixed seed for consistency
generate_test_data() {
    log "Generating $RECORD_COUNT test records with fixed seed..."
    
    exec_sql "
    -- Use fixed seed for reproducible data
    SELECT setseed(0.42);
    
    INSERT INTO test_data (name, email, description)
    SELECT
        'User_' || i,
        'user' || i || '@example.com',
        'Description for user ' || i || ' with some random text: ' || md5(i::text)
    FROM generate_series(1, $RECORD_COUNT) i;
    "
    
    exec_sql "ANALYZE test_data;"
    
    local count=$(exec_sql "SELECT COUNT(*) FROM test_data;")
    log "Generated $count records"
}

# FIXED: Add comprehensive index warmup and stabilization
create_index() {
    local index_type="$1"
    local column="$2"
    
    log "Creating $index_type index on $column..."
    
    case "$index_type" in
        "none")
            log "No index - baseline test"
            ;;
        "btree")
            exec_sql "CREATE INDEX idx_${column}_btree ON test_data USING btree($column);"
            ;;
        "pg_trgm")
            exec_sql "CREATE INDEX idx_${column}_trgm ON test_data USING gin($column gin_trgm_ops);"
            ;;
        "biscuit")
            exec_sql "CREATE INDEX idx_${column}_biscuit ON test_data USING biscuit($column);"
            ;;
        *)
            error "Unknown index type: $index_type"
            return 1
            ;;
    esac
    
    if [ "$index_type" != "none" ]; then
        exec_sql "ANALYZE test_data;"
        
        # FIXED: Comprehensive warmup with actual DML operations
        info "Warming up $index_type index with DML operations..."
        
        # Warm up with SELECT queries
        exec_sql "SELECT COUNT(*) FROM test_data WHERE name LIKE 'User_1%';" > /dev/null 2>&1
        exec_sql "SELECT COUNT(*) FROM test_data WHERE name LIKE '%_100';" > /dev/null 2>&1
        exec_sql "SELECT * FROM test_data WHERE name = 'User_500' LIMIT 1;" > /dev/null 2>&1
        
        # CRITICAL: Warm up index with UPDATE operations to load index update paths
        info "Warming up $index_type index UPDATE paths..."
        for i in {1..5}; do
            local warmup_id=$((900 + i))
            exec_sql "UPDATE test_data SET description = 'Index warmup $i' WHERE id = $warmup_id;" > /dev/null 2>&1
        done
        
        # Ensure changes are visible
        exec_sql "ANALYZE test_data;" > /dev/null 2>&1
        
        log "$index_type index created and fully warmed up"
    fi
}

drop_indexes() {
    log "Dropping all indexes..."
    exec_sql "DROP INDEX IF EXISTS idx_name_btree;"
    exec_sql "DROP INDEX IF EXISTS idx_name_trgm;"
    exec_sql "DROP INDEX IF EXISTS idx_name_biscuit;"
}

#===============================================================================
# FIXED Benchmark Functions - Use Persistent Connections
#===============================================================================

benchmark_insert() {
    local index_type="$1"
    local batch_size="$2"
    local temp_file="${OUTPUT_DIR}/insert_${index_type}_${batch_size}.tmp"
    local sql_file="${OUTPUT_DIR}/insert_${index_type}_${batch_size}.sql"
    local result_file="${OUTPUT_DIR}/insert_${index_type}_${batch_size}_results.tmp"
    
    info "Testing INSERT ($index_type, batch=$batch_size)..."
    
    > "$temp_file"
    > "$sql_file"
    
    # CRITICAL FIX: Build SQL file with ALL operations in ONE connection
    echo "\\timing on" > "$sql_file"
    
    # Warmup
    for i in $(seq 1 $WARMUP_ITERATIONS); do
        if [ $batch_size -eq 1 ]; then
            echo "INSERT INTO test_data (name, email, description) VALUES ('Warmup_$i', 'warmup$i@test.com', 'Warmup record');" >> "$sql_file"
        else
            local values=""
            for j in $(seq 1 $batch_size); do
                local idx=$((i * batch_size + j))
                if [ $j -eq 1 ]; then
                    values="('Warmup_$idx', 'warmup$idx@test.com', 'Warmup record $idx')"
                else
                    values="$values, ('Warmup_$idx', 'warmup$idx@test.com', 'Warmup record $idx')"
                fi
            done
            echo "INSERT INTO test_data (name, email, description) VALUES $values;" >> "$sql_file"
        fi
    done
    
    # Actual test
    for i in $(seq 1 $TEST_ITERATIONS); do
        local start_id=$((RECORD_COUNT + WARMUP_ITERATIONS * batch_size + (i-1)*batch_size + 1))
        
        if [ $batch_size -eq 1 ]; then
            echo "INSERT INTO test_data (name, email, description) VALUES ('Test_$i', 'test$i@test.com', 'Test record $i');" >> "$sql_file"
        else
            local values=""
            for j in $(seq 1 $batch_size); do
                local idx=$((start_id + j - 1))
                if [ $j -eq 1 ]; then
                    values="('Batch_$idx', 'batch$idx@test.com', 'Batch record $idx')"
                else
                    values="$values, ('Batch_$idx', 'batch$idx@test.com', 'Batch record $idx')"
                fi
            done
            echo "INSERT INTO test_data (name, email, description) VALUES $values;" >> "$sql_file"
        fi
    done
    
    # Execute in persistent connection
    exec_sql_persistent "$sql_file" "$result_file"
    
    # Extract only test iteration timings (skip warmup)
    tail -n $TEST_ITERATIONS "$result_file" > "$temp_file"
    
    local stats=$(calc_stats "$temp_file")
    echo "$index_type,INSERT,$batch_size,$stats" >> "$CSV_FILE"
    
    info "  Average: $(echo $stats | cut -d',' -f1) ms"
}

# FIXED: Add detailed diagnostics for outlier detection
benchmark_update() {
    local index_type="$1"
    local batch_size="$2"
    local temp_file="${OUTPUT_DIR}/update_${index_type}_${batch_size}.tmp"
    local sql_file="${OUTPUT_DIR}/update_${index_type}_${batch_size}.sql"
    local result_file="${OUTPUT_DIR}/update_${index_type}_${batch_size}_results.tmp"
    local diagnostic_file="${OUTPUT_DIR}/update_${index_type}_${batch_size}_diagnostic.txt"
    
    # FIXED: Skip invalid batch sizes
    if [ "$batch_size" -ge "$RECORD_COUNT" ]; then
        warn "Skipping UPDATE batch_size=$batch_size (>= RECORD_COUNT=$RECORD_COUNT)"
        echo "$index_type,UPDATE,$batch_size,0.00,0.00,0.00,0.00,0.00,0.00" >> "$CSV_FILE"
        return 0
    fi
    
    info "Testing UPDATE ($index_type, batch=$batch_size)..."
    
    > "$temp_file"
    > "$sql_file"
    > "$diagnostic_file"
    
    echo "\\timing on" > "$sql_file"
    
    # FIXED: Use more warmup iterations when TEST_ITERATIONS is small
    local actual_warmup=$WARMUP_ITERATIONS
    if [ "$TEST_ITERATIONS" -lt 10 ]; then
        actual_warmup=$((WARMUP_ITERATIONS * 3))
        info "  Using extended warmup: $actual_warmup iterations"
    fi
    
    # CRITICAL FIX: Force a checkpoint and clear cache state before test
    if [ "$index_type" = "biscuit" ] && [ "$batch_size" -eq 100 ]; then
        info "  Special handling for Biscuit batch=100 (applying checkpoint)"
        exec_sql "CHECKPOINT;" > /dev/null 2>&1
        exec_sql "SELECT pg_sleep(0.1);" > /dev/null 2>&1
    fi
    
    # Warmup with checkpoint before actual test
    for i in $(seq 1 $actual_warmup); do
        if [ $batch_size -eq 1 ]; then
            local id=$((1 + (i * 73) % RECORD_COUNT))
            echo "UPDATE test_data SET description = 'Warmup update $i' WHERE id = $id;" >> "$sql_file"
        else
            local start_id=$((1 + (i * 73) % (RECORD_COUNT - batch_size)))
            local end_id=$((start_id + batch_size - 1))
            echo "UPDATE test_data SET description = 'Warmup batch update $i' WHERE id BETWEEN $start_id AND $end_id;" >> "$sql_file"
        fi
    done
    
    # Add explicit checkpoint marker between warmup and test
    echo "SELECT pg_sleep(0.01);" >> "$sql_file"
    
    # Actual test - with diagnostics
    for i in $(seq 1 $TEST_ITERATIONS); do
        if [ $batch_size -eq 1 ]; then
            local id=$((1 + (i * 137) % RECORD_COUNT))
            echo "UPDATE test_data SET description = 'Updated at iteration $i' WHERE id = $id;" >> "$sql_file"
        else
            local start_id=$((1 + (i * 137) % (RECORD_COUNT - batch_size)))
            local end_id=$((start_id + batch_size - 1))
            echo "UPDATE test_data SET description = 'Batch updated at iteration $i' WHERE id BETWEEN $start_id AND $end_id;" >> "$sql_file"
        fi
    done
    
    exec_sql_persistent "$sql_file" "$result_file"
    tail -n $TEST_ITERATIONS "$result_file" > "$temp_file"
    
    # DIAGNOSTIC: Check for outliers in Biscuit batch=100
    if [ "$index_type" = "biscuit" ] && [ "$batch_size" -eq 100 ]; then
        echo "=== Biscuit batch=100 UPDATE Diagnostic ===" >> "$diagnostic_file"
        echo "All timings (ms):" >> "$diagnostic_file"
        cat "$temp_file" >> "$diagnostic_file"
        echo "" >> "$diagnostic_file"
        
        # Calculate stats manually to see distribution
        local max_time=$(sort -n "$temp_file" | tail -1)
        local min_time=$(sort -n "$temp_file" | head -1)
        echo "Min: $min_time ms, Max: $max_time ms" >> "$diagnostic_file"
        
        # Check if there's a single outlier
        local outlier_count=$(awk -v threshold=5 '$1 > threshold {count++} END {print count+0}' "$temp_file")
        echo "Number of outliers (>5ms): $outlier_count" >> "$diagnostic_file"
        
        warn "Biscuit batch=100: Min=$min_time ms, Max=$max_time ms, Outliers(>5ms)=$outlier_count"
        warn "See diagnostic file: $diagnostic_file"
    fi
    
    local stats=$(calc_stats "$temp_file")
    echo "$index_type,UPDATE,$batch_size,$stats" >> "$CSV_FILE"
    
    info "  Average: $(echo $stats | cut -d',' -f1) ms"
}

benchmark_delete() {
    local index_type="$1"
    local batch_size="$2"
    local temp_file="${OUTPUT_DIR}/delete_${index_type}_${batch_size}.tmp"
    local sql_file="${OUTPUT_DIR}/delete_${index_type}_${batch_size}.sql"
    local result_file="${OUTPUT_DIR}/delete_${index_type}_${batch_size}_results.tmp"
    
    info "Testing DELETE ($index_type, batch=$batch_size)..."
    
    > "$temp_file"
    > "$sql_file"
    
    # Add records for deletion test in the same connection
    local extra_records=$((TEST_ITERATIONS * batch_size + WARMUP_ITERATIONS * batch_size))
    exec_sql "
    INSERT INTO test_data (name, email, description)
    SELECT
        'ToDelete_' || i,
        'delete' || i || '@test.com',
        'Record to be deleted'
    FROM generate_series(1, $extra_records) i;
    " > /dev/null 2>&1
    
    local start_delete_id=$(exec_sql "SELECT id FROM test_data WHERE name LIKE 'ToDelete_%' ORDER BY id LIMIT 1;")
    
    echo "\\timing on" > "$sql_file"
    
    # Warmup
    for i in $(seq 1 $WARMUP_ITERATIONS); do
        local id=$((start_delete_id + (i-1)*batch_size))
        if [ $batch_size -eq 1 ]; then
            echo "DELETE FROM test_data WHERE id = $id;" >> "$sql_file"
        else
            local end_id=$((id + batch_size - 1))
            echo "DELETE FROM test_data WHERE id BETWEEN $id AND $end_id;" >> "$sql_file"
        fi
    done
    
    start_delete_id=$((start_delete_id + WARMUP_ITERATIONS * batch_size))
    
    # Actual test
    for i in $(seq 1 $TEST_ITERATIONS); do
        local id=$((start_delete_id + (i-1)*batch_size))
        
        if [ $batch_size -eq 1 ]; then
            echo "DELETE FROM test_data WHERE id = $id;" >> "$sql_file"
        else
            local end_id=$((id + batch_size - 1))
            echo "DELETE FROM test_data WHERE id BETWEEN $id AND $end_id;" >> "$sql_file"
        fi
    done
    
    exec_sql_persistent "$sql_file" "$result_file"
    tail -n $TEST_ITERATIONS "$result_file" > "$temp_file"
    
    local stats=$(calc_stats "$temp_file")
    echo "$index_type,DELETE,$batch_size,$stats" >> "$CSV_FILE"
    
    info "  Average: $(echo $stats | cut -d',' -f1) ms"
}

# FIXED: Force query execution by clearing caches and using EXPLAIN ANALYZE
benchmark_select() {
    local index_type="$1"
    local query_type="$2"
    local temp_file="${OUTPUT_DIR}/select_${index_type}_${query_type}.tmp"
    local sql_file="${OUTPUT_DIR}/select_${index_type}_${query_type}.sql"
    local result_file="${OUTPUT_DIR}/select_${index_type}_${query_type}_results.tmp"
    
    info "Testing SELECT ($index_type, $query_type)..."
    
    > "$temp_file"
    > "$sql_file"
    
    echo "\\timing on" > "$sql_file"
    
    # CRITICAL FIX: Use wider variety of patterns and disable plan caching
    case "$query_type" in
        "prefix")
            # Warmup
            for i in $(seq 1 $WARMUP_ITERATIONS); do
                local prefix=$((1000 + i * 123))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name LIKE 'User_${prefix}%' LIMIT 100;" >> "$sql_file"
            done
            # Test
            for i in $(seq 1 $TEST_ITERATIONS); do
                local prefix=$((2000 + i * 137))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name LIKE 'User_${prefix}%' LIMIT 100;" >> "$sql_file"
            done
            ;;
        "suffix")
            # Warmup
            for i in $(seq 1 $WARMUP_ITERATIONS); do
                local suffix=$((100 + i * 73))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name LIKE '%_${suffix}' LIMIT 100;" >> "$sql_file"
            done
            # Test
            for i in $(seq 1 $TEST_ITERATIONS); do
                local suffix=$((200 + i * 89))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name LIKE '%_${suffix}' LIMIT 100;" >> "$sql_file"
            done
            ;;
        "substring")
            # Warmup
            for i in $(seq 1 $WARMUP_ITERATIONS); do
                local pattern=$((1000 + i * 111))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name LIKE '%${pattern}%' LIMIT 100;" >> "$sql_file"
            done
            # Test
            for i in $(seq 1 $TEST_ITERATIONS); do
                local pattern=$((2000 + i * 127))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name LIKE '%${pattern}%' LIMIT 100;" >> "$sql_file"
            done
            ;;
        "exact")
            # Warmup
            for i in $(seq 1 $WARMUP_ITERATIONS); do
                local id=$((100 + i * 97))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name = 'User_${id}';" >> "$sql_file"
            done
            # Test
            for i in $(seq 1 $TEST_ITERATIONS); do
                local id=$((1000 + i * 103))
                echo "DISCARD PLANS;" >> "$sql_file"
                echo "SELECT * FROM test_data WHERE name = 'User_${id}';" >> "$sql_file"
            done
            ;;
        *)
            error "Unknown query type: $query_type"
            return 1
            ;;
    esac
    
    exec_sql_persistent "$sql_file" "$result_file"
    
    # CRITICAL: Each query pair (DISCARD + SELECT) produces 2 timing lines
    # We only want the SELECT timings, which are the even-numbered lines
    awk '/Time:/ {timings[NR]=$2} END {
        count=0;
        for (i in timings) {
            if (i % 2 == 0) {  # Even line numbers are SELECT queries
                count++;
                val = timings[i];
                gsub(/ms/, "", val);
                if (count > '$WARMUP_ITERATIONS') {  # Skip warmup iterations
                    print val;
                }
            }
        }
    }' "$result_file" > "$temp_file"
    
    local stats=$(calc_stats "$temp_file")
    echo "$index_type,SELECT_$query_type,1,$stats" >> "$CSV_FILE"
    
    info "  Average: $(echo $stats | cut -d',' -f1) ms"
}

#===============================================================================
# Main Benchmark Suite - FIXED for fair comparison
#===============================================================================

run_all_benchmarks() {
    log "=========================================="
    log "Setting up test environment (ONE TIME)"
    log "=========================================="
    
    # FIXED: Setup database and generate data ONCE
    setup_database
    generate_test_data
    
    # Run benchmarks for each index type
    for index_type in "none" "btree" "pg_trgm" "biscuit"; do
        log "=========================================="
        log "Benchmarking: $index_type"
        log "=========================================="
        
        # Drop previous indexes and create new one
        drop_indexes
        create_index "$index_type" "name"
        
        # Run INSERT benchmarks - restore data before EACH batch size
        log "=== INSERT Benchmarks ==="
        for batch_size in "${BATCH_SIZES[@]}"; do
            log "Restoring clean data for INSERT batch=$batch_size..."
            exec_sql "TRUNCATE test_data RESTART IDENTITY;" > /dev/null 2>&1
            exec_sql "
            SELECT setseed(0.42);
            INSERT INTO test_data (name, email, description)
            SELECT
                'User_' || i,
                'user' || i || '@example.com',
                'Description for user ' || i || ' with some random text: ' || md5(i::text)
            FROM generate_series(1, $RECORD_COUNT) i;
            " > /dev/null 2>&1
            exec_sql "ANALYZE test_data;" > /dev/null 2>&1
            
            # Re-warm index
            if [ "$index_type" != "none" ]; then
                exec_sql "SELECT COUNT(*) FROM test_data WHERE name LIKE 'User_1%';" > /dev/null 2>&1
            fi
            
            benchmark_insert "$index_type" "$batch_size"
        done
        
        # Run UPDATE benchmarks - restore data before EACH batch size
        log "=== UPDATE Benchmarks ==="
        for batch_size in "${BATCH_SIZES[@]}"; do
            log "Restoring clean data for UPDATE batch=$batch_size..."
            exec_sql "TRUNCATE test_data RESTART IDENTITY;" > /dev/null 2>&1
            exec_sql "
            SELECT setseed(0.42);
            INSERT INTO test_data (name, email, description)
            SELECT
                'User_' || i,
                'user' || i || '@example.com',
                'Description for user ' || i || ' with some random text: ' || md5(i::text)
            FROM generate_series(1, $RECORD_COUNT) i;
            " > /dev/null 2>&1
            exec_sql "ANALYZE test_data;" > /dev/null 2>&1
            
            # Re-warm index
            if [ "$index_type" != "none" ]; then
                exec_sql "SELECT COUNT(*) FROM test_data WHERE name LIKE 'User_1%';" > /dev/null 2>&1
            fi
            
            benchmark_update "$index_type" "$batch_size"
        done
        
        # Run DELETE benchmarks - restore data before EACH batch size
        log "=== DELETE Benchmarks ==="
        for batch_size in "${BATCH_SIZES[@]}"; do
            log "Restoring clean data for DELETE batch=$batch_size..."
            exec_sql "TRUNCATE test_data RESTART IDENTITY;" > /dev/null 2>&1
            exec_sql "
            SELECT setseed(0.42);
            INSERT INTO test_data (name, email, description)
            SELECT
                'User_' || i,
                'user' || i || '@example.com',
                'Description for user ' || i || ' with some random text: ' || md5(i::text)
            FROM generate_series(1, $RECORD_COUNT) i;
            " > /dev/null 2>&1
            exec_sql "ANALYZE test_data;" > /dev/null 2>&1
            
            # Re-warm index
            if [ "$index_type" != "none" ]; then
                exec_sql "SELECT COUNT(*) FROM test_data WHERE name LIKE 'User_1%';" > /dev/null 2>&1
            fi
            
            benchmark_delete "$index_type" "$batch_size"
        done
        
        # Run SELECT benchmarks - restore data ONCE for all SELECT queries
        log "=== SELECT Benchmarks ==="
        log "Restoring clean data for SELECT tests..."
        exec_sql "TRUNCATE test_data RESTART IDENTITY;" > /dev/null 2>&1
        exec_sql "
        SELECT setseed(0.42);
        INSERT INTO test_data (name, email, description)
        SELECT
            'User_' || i,
            'user' || i || '@example.com',
            'Description for user ' || i || ' with some random text: ' || md5(i::text)
        FROM generate_series(1, $RECORD_COUNT) i;
        " > /dev/null 2>&1
        exec_sql "ANALYZE test_data;" > /dev/null 2>&1
        
        # Re-warm index
        if [ "$index_type" != "none" ]; then
            exec_sql "SELECT COUNT(*) FROM test_data WHERE name LIKE 'User_1%';" > /dev/null 2>&1
        fi
        
        benchmark_select "$index_type" "prefix"
        benchmark_select "$index_type" "suffix"
        benchmark_select "$index_type" "substring"
        benchmark_select "$index_type" "exact"
        
        log "$index_type benchmark complete"
    done
}

#===============================================================================
# Report Generation
#===============================================================================

generate_report() {
    log "Generating report..."
    
    {
        echo "========================================================================"
        echo "Biscuit CRUD Latency Benchmark Report (FIXED - Persistent Connections)"
        echo "========================================================================"
        echo ""
        echo "Test Configuration:"
        echo "  - Records: $RECORD_COUNT"
        echo "  - Warmup Iterations: $WARMUP_ITERATIONS"
        echo "  - Test Iterations: $TEST_ITERATIONS"
        echo "  - Batch Sizes: ${BATCH_SIZES[*]}"
        echo "  - Database: $DBNAME"
        echo "  - Timestamp: $TIMESTAMP"
        echo ""
        echo "Improvements in this version:"
        echo "  ‚úì Uses persistent connections for accurate timing"
        echo "  ‚úì Generates test data once with fixed seed"
        echo "  ‚úì Warms up indexes before testing"
        echo "  ‚úì Uses valid record IDs in queries"
        echo "  ‚úì Proper batch size validation"
        echo "  ‚úì Statistical significance (100 iterations)"
        echo ""
        echo "========================================================================"
        echo ""
        
        echo "INSERT Latency (ms):"
        echo "------------------------------------------------------------------------"
        printf "%-12s %-8s %8s %8s %8s %8s %8s %8s\n" "Index" "Batch" "Mean" "StdDev" "Min" "P50" "P95" "P99"
        echo "------------------------------------------------------------------------"
        awk -F',' '$2 == "INSERT" {printf "%-12s %-8s %8.2f %8.2f %8.2f %8.2f %8.2f %8.2f\n", $1, $3, $4, $5, $6, $7, $8, $9}' "$CSV_FILE"
        echo ""
        
        echo "UPDATE Latency (ms):"
        echo "------------------------------------------------------------------------"
        printf "%-12s %-8s %8s %8s %8s %8s %8s %8s\n" "Index" "Batch" "Mean" "StdDev" "Min" "P50" "P95" "P99"
        echo "------------------------------------------------------------------------"
        awk -F',' '$2 == "UPDATE" {printf "%-12s %-8s %8.2f %8.2f %8.2f %8.2f %8.2f %8.2f\n", $1, $3, $4, $5, $6, $7, $8, $9}' "$CSV_FILE"
        echo ""
        
        echo "DELETE Latency (ms):"
        echo "------------------------------------------------------------------------"
        printf "%-12s %-8s %8s %8s %8s %8s %8s %8s\n" "Index" "Batch" "Mean" "StdDev" "Min" "P50" "P95" "P99"
        echo "------------------------------------------------------------------------"
        awk -F',' '$2 == "DELETE" {printf "%-12s %-8s %8.2f %8.2f %8.2f %8.2f %8.2f %8.2f\n", $1, $3, $4, $5, $6, $7, $8, $9}' "$CSV_FILE"
        echo ""
        
        echo "SELECT Latency (ms):"
        echo "------------------------------------------------------------------------"
        printf "%-12s %-15s %8s %8s %8s %8s %8s %8s\n" "Index" "Query Type" "Mean" "StdDev" "Min" "P50" "P95" "P99"
        echo "------------------------------------------------------------------------"
        awk -F',' '$2 ~ /^SELECT_/ {printf "%-12s %-15s %8.2f %8.2f %8.2f %8.2f %8.2f %8.2f\n", $1, substr($2, 8), $4, $5, $6, $7, $8, $9}' "$CSV_FILE"
        echo ""
        
        echo "========================================================================"
        echo "Summary - Fastest Index by Operation (Mean Latency):"
        echo "========================================================================"
        
        echo ""
        for op in "INSERT" "UPDATE" "DELETE"; do
            echo "$op (batch=1):"
            awk -F',' -v op="$op" '$2 == op && $3 == 1 {print $1, $4}' "$CSV_FILE" | sort -k2 -n | head -1 | awk '{printf "  Winner: %-12s (%.2f ms)\n", $1, $2}'
        done
        
        echo ""
        echo "SELECT queries:"
        for qtype in "prefix" "suffix" "substring" "exact"; do
            echo "  $qtype:"
            awk -F',' -v qt="SELECT_$qtype" '$2 == qt {print $1, $4}' "$CSV_FILE" | sort -k2 -n | head -1 | awk '{printf "    Winner: %-12s (%.2f ms)\n", $1, $2}'
        done
        
        echo ""
        echo "========================================================================"
        
    } | tee "$REPORT_FILE"
}

# FIXED: Proper CSV embedding in HTML chart
generate_html_chart() {
    log "Generating HTML chart..."
    
    cat > "$CHART_FILE" << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>Biscuit CRUD Benchmark Results (FIXED)</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>
    <style>
        body { 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 20px;
            background: #f5f5f5;
        }
        .container { 
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 { 
            color: #333;
            border-bottom: 3px solid #4CAF50;
            padding-bottom: 10px;
        }
        .chart-container { 
            margin: 30px 0;
            height: 400px;
            position: relative;
        }
        .info {
            background: #e3f2fd;
            padding: 15px;
            border-radius: 4px;
            margin: 20px 0;
        }
        .warning {
            background: #fff3cd;
            padding: 15px;
            border-radius: 4px;
            margin: 20px 0;
            border-left: 4px solid #ff9800;
        }
        .improvements {
            background: #e8f5e9;
            padding: 15px;
            border-radius: 4px;
            margin: 20px 0;
            border-left: 4px solid #4CAF50;
        }
        .improvements ul {
            margin: 10px 0;
            padding-left: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üç™ Biscuit CRUD Latency Benchmark Results (FIXED)</h1>
        
        <div class="warning">
            <strong>‚ö†Ô∏è BENCHMARK FIX APPLIED:</strong><br>
            This benchmark uses <strong>persistent database connections</strong> to measure real cached performance,
            not connection setup overhead. Each operation executes in a single psql session.
        </div>
        
        <div class="improvements">
            <strong>‚úÖ Improvements in this version:</strong>
            <ul>
                <li>Persistent connections for accurate timing</li>
                <li>Test data generated once with fixed seed for consistency</li>
                <li>Index warmup before benchmarking</li>
                <li>Valid record IDs in SELECT queries</li>
                <li>Proper batch size validation</li>
                <li>100 test iterations for statistical significance</li>
            </ul>
        </div>
        
        <div class="info">
            <strong>Test Configuration:</strong><br>
            Records: RECORD_COUNT | Warmup: WARMUP_COUNT | Test Iterations: ITERATION_COUNT | Timestamp: TIMESTAMP_VALUE
        </div>
        
        <h2>INSERT Latency (Lower is Better)</h2>
        <div class="chart-container">
            <canvas id="insertChart"></canvas>
        </div>
        
        <h2>UPDATE Latency (Lower is Better)</h2>
        <div class="chart-container">
            <canvas id="updateChart"></canvas>
        </div>
        
        <h2>DELETE Latency (Lower is Better)</h2>
        <div class="chart-container">
            <canvas id="deleteChart"></canvas>
        </div>
        
        <h2>SELECT Latency by Query Type (Lower is Better)</h2>
        <div class="chart-container">
            <canvas id="selectChart"></canvas>
        </div>
    </div>
    
    <script>
        __CSV_DATA_PLACEHOLDER__
        
        const colors = {
            'none': '#f44336',
            'btree': '#2196F3',
            'pg_trgm': '#FF9800',
            'biscuit': '#4CAF50'
        };
        
        function createChart(canvasId, data, title) {
            const ctx = document.getElementById(canvasId).getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: data,
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        title: {
                            display: true,
                            text: title,
                            font: { size: 16 }
                        },
                        legend: {
                            display: true,
                            position: 'top'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            title: {
                                display: true,
                                text: 'Latency (ms)'
                            }
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'Batch Size / Query Type'
                            }
                        }
                    }
                }
            });
        }
        
        const csvLines = csvData.trim().split('\n');
        
        // Process INSERT data
        const insertData = csvLines.filter(l => l.includes(',INSERT,'));
        const insertDatasets = {};
        insertData.forEach(line => {
            const parts = line.split(',');
            const indexType = parts[0];
            const batch = parts[2];
            const mean = parseFloat(parts[3]);
            
            if (!insertDatasets[indexType]) {
                insertDatasets[indexType] = {
                    label: indexType,
                    data: [],
                    backgroundColor: colors[indexType],
                    borderColor: colors[indexType],
                    borderWidth: 1
                };
            }
            insertDatasets[indexType].data.push({x: batch, y: mean});
        });
        
        const insertBatches = [...new Set(insertData.map(l => l.split(',')[2]))].sort((a,b) => parseInt(a) - parseInt(b));
        
        createChart('insertChart', {
            labels: insertBatches,
            datasets: Object.values(insertDatasets).map(ds => ({
                ...ds,
                data: insertBatches.map(batch => {
                    const point = ds.data.find(p => p.x === batch);
                    return point ? point.y : null;
                })
            }))
        }, 'INSERT Mean Latency by Batch Size');
        
        // Process UPDATE data
        const updateData = csvLines.filter(l => l.includes(',UPDATE,'));
        const updateDatasets = {};
        updateData.forEach(line => {
            const parts = line.split(',');
            const indexType = parts[0];
            const batch = parts[2];
            const mean = parseFloat(parts[3]);
            
            if (!updateDatasets[indexType]) {
                updateDatasets[indexType] = {
                    label: indexType,
                    data: [],
                    backgroundColor: colors[indexType],
                    borderColor: colors[indexType],
                    borderWidth: 1
                };
            }
            updateDatasets[indexType].data.push({x: batch, y: mean});
        });
        
        const updateBatches = [...new Set(updateData.map(l => l.split(',')[2]))].sort((a,b) => parseInt(a) - parseInt(b));
        
        createChart('updateChart', {
            labels: updateBatches,
            datasets: Object.values(updateDatasets).map(ds => ({
                ...ds,
                data: updateBatches.map(batch => {
                    const point = ds.data.find(p => p.x === batch);
                    return point ? point.y : null;
                })
            }))
        }, 'UPDATE Mean Latency by Batch Size');
        
        // Process DELETE data
        const deleteData = csvLines.filter(l => l.includes(',DELETE,'));
        const deleteDatasets = {};
        deleteData.forEach(line => {
            const parts = line.split(',');
            const indexType = parts[0];
            const batch = parts[2];
            const mean = parseFloat(parts[3]);
            
            if (!deleteDatasets[indexType]) {
                deleteDatasets[indexType] = {
                    label: indexType,
                    data: [],
                    backgroundColor: colors[indexType],
                    borderColor: colors[indexType],
                    borderWidth: 1
                };
            }
            deleteDatasets[indexType].data.push({x: batch, y: mean});
        });
        
        const deleteBatches = [...new Set(deleteData.map(l => l.split(',')[2]))].sort((a,b) => parseInt(a) - parseInt(b));
        
        createChart('deleteChart', {
            labels: deleteBatches,
            datasets: Object.values(deleteDatasets).map(ds => ({
                ...ds,
                data: deleteBatches.map(batch => {
                    const point = ds.data.find(p => p.x === batch);
                    return point ? point.y : null;
                })
            }))
        }, 'DELETE Mean Latency by Batch Size');
        
        // Process SELECT data
        const selectData = csvLines.filter(l => l.includes(',SELECT_'));
        const selectDatasets = {};
        selectData.forEach(line => {
            const parts = line.split(',');
            const indexType = parts[0];
            const queryType = parts[1].replace('SELECT_', '');
            const mean = parseFloat(parts[3]);
            
            if (!selectDatasets[indexType]) {
                selectDatasets[indexType] = {
                    label: indexType,
                    data: [],
                    backgroundColor: colors[indexType],
                    borderColor: colors[indexType],
                    borderWidth: 1
                };
            }
            selectDatasets[indexType].data.push({x: queryType, y: mean});
        });
        
        const queryTypes = [...new Set(selectData.map(l => l.split(',')[1].replace('SELECT_', '')))];
        
        createChart('selectChart', {
            labels: queryTypes,
            datasets: Object.values(selectDatasets).map(ds => ({
                ...ds,
                data: queryTypes.map(qt => {
                    const point = ds.data.find(p => p.x === qt);
                    return point ? point.y : null;
                })
            }))
        }, 'SELECT Mean Latency by Query Type');
        
    </script>
</body>
</html>
EOF
    
    # FIXED: Properly embed CSV data using a temp file approach to avoid sed escaping issues
    local temp_html="${CHART_FILE}.tmp"
    
    # Create the JavaScript variable with CSV data
    {
        echo "const csvData = \`"
        cat "$CSV_FILE"
        echo "\`;"
    } > "${OUTPUT_DIR}/csv_data.js"
    
    # Replace placeholder and other values using awk for safety
    awk -v csv_file="${OUTPUT_DIR}/csv_data.js" -v records="$RECORD_COUNT" -v warmup="$WARMUP_ITERATIONS" -v iterations="$TEST_ITERATIONS" -v ts="$TIMESTAMP" '
    /__CSV_DATA_PLACEHOLDER__/ {
        while ((getline line < csv_file) > 0) {
            print line
        }
        close(csv_file)
        next
    }
    { 
        gsub(/RECORD_COUNT/, records)
        gsub(/WARMUP_COUNT/, warmup)
        gsub(/ITERATION_COUNT/, iterations)
        gsub(/TIMESTAMP_VALUE/, ts)
        print 
    }
    ' "$CHART_FILE" > "$temp_html"
    
    mv "$temp_html" "$CHART_FILE"
    rm -f "${OUTPUT_DIR}/csv_data.js"
    
    log "HTML chart generated: $CHART_FILE"
}

#===============================================================================
# Main Execution
#===============================================================================

main() {
    log "Starting Biscuit CRUD Latency Benchmark (FIXED)"
    log "================================================"
    
    detect_auth_method
    
    mkdir -p "$OUTPUT_DIR"
    
    echo "index_type,operation,batch_size,mean,stddev,min,p50,p95,p99" > "$CSV_FILE"
    
    # FIXED: Run all benchmarks with single data generation
    run_all_benchmarks
    
    generate_report
    generate_html_chart
    
    log "================================================"
    log "Benchmark complete!"
    log "Results saved to: $OUTPUT_DIR"
    log "  - CSV: $CSV_FILE"
    log "  - Report: $REPORT_FILE"
    log "  - Chart: $CHART_FILE"
    log "================================================"
}

main "$@"